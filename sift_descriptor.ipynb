{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preprocess.image_matching_dataset import ImageMatchingDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "from feature_descriptors.sift_descriptor import get_keypoint_and_descriptor\n",
    "from feature_matching.flann_matcher import flann_matcher\n",
    "from load_h5py_files import load_sift_output, load_flann_output, load_flann_from_images_name\n",
    "from clustering.cluster_images import build_graph, graph_clustering\n",
    "from batch_descriptor import batch_feature_descriptor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import pad\n",
    "\n",
    "class PadToSize():\n",
    "    def __init__(self, size, fill=0):\n",
    "        self.size = size\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        pad_h = max(0, self.size - h)\n",
    "        pad_w = max(0, self.size - w)\n",
    "        padding = [pad_w//2, pad_h//2, pad_w - pad_w//2, pad_h - pad_h//2]\n",
    "        return pad(img, padding, fill=self.fill)\n",
    "\n",
    "        \n",
    "'''Original dataset, no resizing performed, does not work with dataloader since it requires all images of same dimensions'''\n",
    "train_dataset = ImageMatchingDataset(labels_path='data/train_labels.csv', root_dir='data/train',\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor()\n",
    "                               ]))\n",
    "\n",
    "'''train_dataset_resize = ImageMatchingDataset(labels_path='data/train_labels.csv', root_dir='data/train',\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize((1024, 1024)),\n",
    "                                   transforms.Pad((0, 0, 1024, 1024), fill=0),\n",
    "                                   transforms.ToTensor()\n",
    "                               ]))'''\n",
    "\n",
    "\n",
    "'''Dataset with resizing and padding if needed, to be used with dataloader to work in batches'''\n",
    "train_dataset_eq_size = ImageMatchingDataset(labels_path='data/train_labels.csv', root_dir='data/train',\n",
    "                               transform=transforms.Compose([\n",
    "                                   PadToSize(1024, fill=0),\n",
    "                                   transforms.Resize((1024, 1024), antialias=True),\n",
    "                                   transforms.ToTensor()\n",
    "                               ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[0.6902, 0.6863, 0.6902,  ..., 0.2980, 0.2941, 0.2941],\n",
       "          [0.6863, 0.6824, 0.6784,  ..., 0.3020, 0.2941, 0.2941],\n",
       "          [0.7333, 0.7294, 0.7294,  ..., 0.3294, 0.3294, 0.3373],\n",
       "          ...,\n",
       "          [0.3647, 0.3725, 0.3686,  ..., 0.5137, 0.4824, 0.5412],\n",
       "          [0.3451, 0.3569, 0.3608,  ..., 0.5137, 0.5059, 0.4784],\n",
       "          [0.3255, 0.3451, 0.3608,  ..., 0.5333, 0.5176, 0.4078]],\n",
       " \n",
       "         [[0.7059, 0.7020, 0.7059,  ..., 0.4392, 0.4353, 0.4353],\n",
       "          [0.6980, 0.6902, 0.6902,  ..., 0.4431, 0.4353, 0.4353],\n",
       "          [0.7412, 0.7373, 0.7373,  ..., 0.4706, 0.4706, 0.4784],\n",
       "          ...,\n",
       "          [0.5020, 0.5098, 0.5059,  ..., 0.5059, 0.4745, 0.5333],\n",
       "          [0.4824, 0.4941, 0.4980,  ..., 0.5059, 0.4980, 0.4706],\n",
       "          [0.4627, 0.4824, 0.4980,  ..., 0.5255, 0.5098, 0.4000]],\n",
       " \n",
       "         [[0.7020, 0.6980, 0.7020,  ..., 0.1569, 0.1529, 0.1529],\n",
       "          [0.6941, 0.6863, 0.6863,  ..., 0.1608, 0.1529, 0.1529],\n",
       "          [0.7373, 0.7333, 0.7333,  ..., 0.1882, 0.1882, 0.1961],\n",
       "          ...,\n",
       "          [0.5176, 0.5255, 0.5216,  ..., 0.4235, 0.3922, 0.4510],\n",
       "          [0.4980, 0.5098, 0.5137,  ..., 0.4235, 0.4157, 0.3882],\n",
       "          [0.4784, 0.4980, 0.5137,  ..., 0.4431, 0.4275, 0.3176]]]),\n",
       " 'dataset_name': 'imc2023_haiper',\n",
       " 'scene_name': 'fountain',\n",
       " 'image_name': 'fountain_image_116.png',\n",
       " 'image_path': 'data/train/imc2023_haiper/fountain_image_116.png',\n",
       " 'rotation_matrix': tensor([[ 0.1227,  0.9477, -0.2946],\n",
       "         [ 0.1227,  0.2801,  0.9521],\n",
       "         [ 0.9848, -0.1529, -0.0819]]),\n",
       " 'translation_vector': tensor([[ 0.0938],\n",
       "         [-0.8036],\n",
       "         [ 2.0620]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_eq_size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "DEVICE = torch.device(0 if torch.cuda.is_available() else 'cpu')\n",
    "train_loader = DataLoader(train_dataset_eq_size, batch_size=1, shuffle=True, num_workers=10, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_feature_descriptor(train_loader, DEVICE, \"sift\", \"evaluation/sift_descriptors_outputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name\n",
      "descriptors\n",
      "image_name\n",
      "image_path\n",
      "keypoint_scores\n",
      "keypoints\n",
      "scene_name\n",
      "[[[0.15955846 0.07338621 0.03498551 ... 0.02709965 0.05419931 0.09123103]\n",
      "  [0.02835608 0.02315265 0.05177089 ... 0.06945793 0.04331463 0.04331463]\n",
      "  [0.11931757 0.15133567 0.03047096 ... 0.03518483 0.02487943 0.01759242]\n",
      "  ...\n",
      "  [0.05027858 0.00099999 0.02687503 ... 0.04654892 0.02687503 0.0600944 ]\n",
      "  [0.06653827 0.00099999 0.00099999 ... 0.06872043 0.01718011 0.06653827]\n",
      "  [0.0734479  0.06304731 0.05588546 ... 0.06526016 0.03767797 0.02918523]]]\n",
      "[[[585.6262  571.4224 ]\n",
      "  [583.6034  665.56195]\n",
      "  [583.29895 501.2842 ]\n",
      "  ...\n",
      "  [345.56717 623.4795 ]\n",
      "  [666.94116 831.7335 ]\n",
      "  [465.9309  668.9876 ]]]\n",
      "['peach_0058.png']\n",
      "['amy_gardens']\n",
      "['peach']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def print_h5_structure(name, obj):\n",
    "    print(name)\n",
    "\n",
    "with h5py.File(\"evaluation/sift_descriptors_outputs/0_1024_sift.h5\", \"r\") as f:\n",
    "    f.visititems(print_h5_structure)\n",
    "    print(f[\"descriptors\"][:])\n",
    "    print(f[\"keypoints\"][:])\n",
    "    print(f[\"image_name\"].asstr()[:])\n",
    "    print(f[\"dataset_name\"].asstr()[:])\n",
    "    print(f['scene_name'].asstr()[:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlrv",
   "language": "python",
   "name": "dlrv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
